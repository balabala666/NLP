{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问答系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取文件，并把内容分别写到两个list里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def read_corpus(filepath):\n",
    "    with open(filepath) as f:\n",
    "            data = json.load(f)\n",
    "    qlist = list()\n",
    "    alist = list() \n",
    "    for item in data['data']:\n",
    "        for para in item['paragraphs']:\n",
    "            for qa in para['qas']:  \n",
    "                qlist.append(qa['question'])\n",
    "                # 部分answers的list为空，所以会引发IndexError\n",
    "                try:\n",
    "                    alist.append(qa['answers'][0]['text'])\n",
    "                except IndexError:\n",
    "                    qlist.pop()\n",
    "    assert len(qlist) == len(alist)  # 确保长度一样\n",
    "    return qlist, alist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "问答数量：86821\n",
      "['What was Yangon previously known as?', 'With what Belorussian city does Kathmandu have a relationship?', 'In what year did Kathmandu create its initial international relationship?', 'What is KMC an initialism of?']\n",
      "['in the late 1990s', 'singing and dancing', '2003', 'Houston, Texas']\n"
     ]
    }
   ],
   "source": [
    "qlist, alist = read_corpus('../data/train-v2.0.json')\n",
    "print(\"问答数量：%d\" % len(qlist))\n",
    "print(qlist[-4:])\n",
    "print(alist[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 60960], ['What', 36995], ['of', 33969], ['in', 21767], ['to', 18417], ['was', 17063], ['is', 16197], ['did', 15634], ['what', 13187], ['a', 10753], ['How', 8023], ['Who', 8009], ['and', 7229], ['for', 7175], ['many', 5497], ['are', 5454], ['When', 5367], ['that', 4435], ['were', 4428], ['does', 4331]]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "qlist, alist = read_corpus('../data/train-v2.0.json')\n",
    "word_cut = Counter()\n",
    "for item in qlist:\n",
    "    word_cut.update(item.strip(\".?!\").split())\n",
    "value_sort = sorted(word_cut.values(), reverse=True)\n",
    "plt.subplot(221)\n",
    "plt.plot(value_sort)\n",
    "plt.subplot(222)\n",
    "plt.plot(value_sort[:2000])\n",
    "plt.subplot(223)\n",
    "plt.plot(value_sort[:200])\n",
    "plt.subplot(224)\n",
    "plt.plot(value_sort[:20])\n",
    "plt.show()\n",
    "\n",
    "merge = dict(zip(word_cut.values(),word_cut.keys()))\n",
    "print([[merge[v],v] for v in value_sort[:20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预文本处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['beyonc', 'start', 'becom', 'popular'], ['area', 'beyonc', 'compet', 'wa', 'grow'], ['beyonc', 'leav', 'destini', \"'s\", 'child', 'becom', 'solo', 'singer'], ['citi', 'state', 'beyonc', 'grow'], ['decad', 'beyonc', 'becom', 'famou'], ['group', 'wa', 'lead', 'singer'], ['album', 'made', 'worldwid', 'known', 'artist'], ['manag', 'destini', \"'s\", 'child', 'group'], ['beyoncé', 'rise', 'fame'], ['role', 'beyoncé', 'destini', \"'s\", 'child']]\n",
      "打印稀疏度： 0.9996166194981876\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "sw = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "def text_process(text):\n",
    "    seg = list()\n",
    "    for word in word_tokenize(text):\n",
    "         # 小写化、词干提取\n",
    "        word = ps.stem(word.lower())\n",
    "          # 数值归一\n",
    "         # isdigit:检测字符串是否只由数字组成\n",
    "        word = \"#number\" if word.isdigit() else word\n",
    "         # 去停用词\n",
    "        if len(word)>1 and word not in sw:\n",
    "            seg.append(word)\n",
    "    return seg\n",
    "qlist, alist = read_corpus('../data/train-v2.0.json')\n",
    "word_cut = Counter()\n",
    "qlist_seg = list()\n",
    "for text in qlist:\n",
    "    seg = text_process(text)\n",
    "    qlist_seg.append(seg)\n",
    "    word_cut.update(seg)\n",
    "# print(word_cut.values()) \n",
    "# python sorted函数是对可迭代对象的排序，参数有cmp,key,reverse.\n",
    "# cmp:比较的函数，有两个参数，大于返回1，小于返回-1，等于返回0。\n",
    "# key:用来比较进行排序的元素。\n",
    "# reverse：排序的规则，reverse = True是降序\n",
    "value_sort = sorted(word_cut.values(), reverse=True)\n",
    "min_tf = value_sort[int(math.exp(0.99*math.log(len(word_cut))))]\n",
    "for cur in range(len(qlist_seg)):\n",
    "    qlist_seg[cur] = [word for word in qlist_seg[cur] if word_cut[word] >min_tf]\n",
    "# for in if 连击推导式隐式的将link由str转变为了list,相当于：\n",
    "# for word in qlist_seg[cur]:\n",
    "#     if word_cut[word] > min_tf:\n",
    "#         qlist_seg[cur] = word\n",
    "print(qlist_seg[:10])\n",
    "# 把qlist中的每一个问题字符串转换成tf-idf向量，计算其相似度问题\n",
    "vectorizer = TfidfVectorizer() # 定义一个tf-idf的vectorizer\n",
    "# 结果存放在X矩阵\n",
    "X = vectorizer.fit_transform([' '.join(seg) for seg in qlist_seg])\n",
    "def sparsity(X):\n",
    "#     n=nnz(X)返回矩阵X中的非零元素的数目\n",
    "    return 1.0-X.nnz/float(X.shape[0]*X.shape[1])\n",
    "print(\"打印稀疏度：\",sparsity(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于用户的输入问题，找到相似度最高的TOP5问题，并把5个潜在的答案做返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chengdu Shuangliu International Airport', 'Chengdu Shuangliu International Airport', 'aerodrome with facilities for flights to take off and land', 'newspapers', 'various gaming sites']\n"
     ]
    }
   ],
   "source": [
    "from queue import PriorityQueue\n",
    "def top5results(question):\n",
    "#     给定用户输入的问题 input_q, 返回最有可能的TOP 5问题。这里面需要做到以下几点：\n",
    "#     1. 对于用户的输入 input_q 首先做一系列的预处理，然后再转换成tf-idf向量（利用上面的vectorizer)\n",
    "#     2. 计算跟每个库里的问题之间的相似度\n",
    "#     3. 找出相似度最高的top5问题的答案\n",
    "    q_vector = vectorizer.transform([' '.join(text_process(question))])\n",
    "#     print(q_vector.T * X)\n",
    "    # 计算余弦相似度，tfidf默认l2范数；矩阵乘法\n",
    "    sim = (X * q_vector.T).toarray()\n",
    "#     print(sim)\n",
    "    pq = PriorityQueue()\n",
    "    for cur in range(sim.shape[0]):\n",
    "        pq.put((sim[cur][0],cur))\n",
    "        if len(pq.queue) > 5:\n",
    "            pq.get()\n",
    "    \n",
    "    pq_rank = sorted(pq.queue,reverse = True,key = lambda x:x[0])\n",
    "#     print([x[1] for x in pq_rank])\n",
    "    top_idxs = [x[1] for x in pq_rank]  # top_idxs存放相似度最高的（存在qlist里的）问题的下表 \n",
    "#     print(top_idxs)\n",
    "    return [alist[i] for i in top_idxs]  # 返回相似度最高的问题对应的答案，作为TOP5答案    \n",
    "print(top5results(\"Which airport was shut down?\"))   \n",
    "# print(top5results(\"Which airport is closed?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 倒排表\n",
    "上面的算法，一个最大的缺点是每一个用户问题都需要跟库里的所有的问题都计算相似度。假设我们库里的问题非常多，这将是效率非常低的方法。 这里面一个方案是通过倒排表的方式，先从库里面找到跟当前的输入类似的问题描述。然后针对于这些candidates问题再做余弦相似度的计算。这样会节省大量的时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# 制定一个简单的倒排表\n",
    "invert = defaultdict(set)\n",
    "for cur in range(len(qlist_seg)):\n",
    "    for word in qlist_seg[cur]:\n",
    "        invert[word].add(cur)\n",
    "        \n",
    "def topresults(question):\n",
    "    seg = text_process(question)\n",
    "    candidates = set()\n",
    "    for word in seg:\n",
    "         # 取所有包含任意一个词的文档的并集\n",
    "        candidates = candidates | invert[word]\n",
    "    candidates = list(candidates)\n",
    "    q_vector = vectorizer.transform([' '.join(seg)])\n",
    "    sim = (X[candidates] * q_vector.T).toarray()\n",
    "    \n",
    "    pq = PriorityQueue()\n",
    "    for cur in range(sim.shape[0]):\n",
    "        pq.put((sim[cur][0],candidates[cur]))\n",
    "        if len(pq.queue) > 5:\n",
    "            pq.get()\n",
    "    pq_rank = sorted(pq.queue,reverse = True,key = lambda x:x[0])\n",
    "#     print([x[0] for x in pq_rank])\n",
    "    top_idxs = [x[1] for x in pq_rank]  # top_idxs存放相似度最高的（存在qlist里的）问题的下表 \n",
    "    return [alist[i] for i in top_idxs]  # 返回相似度最高的问题对应的答案，作为TOP5答案    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chengdu Shuangliu International Airport', 'Chengdu Shuangliu International Airport', 'aerodrome with facilities for flights to take off and land', 'newspapers', 'various gaming sites']\n",
      "['Chengdu Shuangliu International Airport', 'Chengdu Shuangliu International Airport', 'aerodrome with facilities for flights to take off and land', 'newspapers', 'various gaming sites']\n",
      "['Myanmar', 'Isabel', 'foreign aid', 'Soviet Union and China', '10 days']\n",
      "['Myanmar', 'Isabel', 'foreign aid', 'Soviet Union and China', '10 days']\n"
     ]
    }
   ],
   "source": [
    "print(topresults(\"Which airport was shut down?\"))  \n",
    "print(top5results(\"Which airport was shut down?\"))\n",
    "print(topresults(\"Which government stopped aid after Hurricane Nargis?\"))  \n",
    "print(top5results(\"Which government stopped aid after Hurricane Nargis?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于词向量的文本表示\n",
    "上面所用到的方法论是基于词袋模型（bag-of-words model）。这样的方法论有两个主要的问题：1. 无法计算词语之间的相似度 2. 稀疏度很高。 接下来我们采用词向量作为文本的表示，下载训练好的d=100(100维)的词向量glove.6B.zip。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x000000000EED9DD8>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\python\\venv\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  from ipykernel import kernelapp as app\n",
      "e:\\python\\venv\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import numpy as np\n",
    "# 将GloVe转为word2vec 介绍：将词汇转化为机器能够读懂的向量\n",
    "_ = glove2word2vec('../data/glove.6B/glove.6B.100d.txt', '../data/glove.6B/glove2word2vec.6B.100d.txt')\n",
    "model = KeyedVectors.load_word2vec_format('../data/glove.6B/glove2word2vec.6B.100d.txt')\n",
    "print(model)\n",
    "def sentence_vectorizer(seg):\n",
    "#     将分词数据转为句向量。\n",
    "#     seg: 分词后的数据    \n",
    "    vector = np.zeros((1,100))\n",
    "    size = len(seg)\n",
    "    for word in seg:\n",
    "        try:\n",
    "            vector += model.wv[word]\n",
    "        except KeyError:\n",
    "            size -= 1\n",
    "    return vector/size\n",
    "X = np.zeros((len(qlist_seg), 100))\n",
    "for cur in range(X.shape[0]):\n",
    "    X[cur] = sentence_vectorizer(qlist_seg[cur])\n",
    "# 计算X每一行的l2范数\n",
    "Xnorm2 = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "X = X / Xnorm2\n",
    "\n",
    "def top5results_vector(question):\n",
    "    seg = text_process(question)\n",
    "    candidates = set()\n",
    "    for word in seg:\n",
    "        candidates = candidates | invert[word]\n",
    "    candidates = list(candidates)\n",
    "    q_vector = sentence_vectorizer(seg)\n",
    "    qnorm2 = np.linalg.norm(q_vector, axis=1, keepdims=True)\n",
    "    q_vector = q_vector / qnorm2\n",
    "    sim = (X[candidates] * q_vector.T)\n",
    "    # 使用优先队列找出top5\n",
    "    pq = PriorityQueue()\n",
    "    for cur in range(sim.shape[0]):\n",
    "        pq.put((sim[cur][0], candidates[cur]))\n",
    "        if len(pq.queue) > 5:\n",
    "            pq.get()    \n",
    "    pq_rank = sorted(pq.queue,reverse = True,key = lambda x:x[0])\n",
    "    top_idxs = [x[1] for x in pq_rank]  # top_idxs存放相似度最高的（存在qlist里的）问题的下表 \n",
    "    return [alist[i] for i in top_idxs]  # 返回相似度最高的问题对应的答案，作为TOP5答案                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\python\\venv\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (17026,100) (100,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-a096a1169c69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# print(top5results(\"Which airport was shut down?\"))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop5results_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Which airport was shut down?\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-30-1224e5a65be8>\u001b[0m in \u001b[0;36mtop5results_vector\u001b[1;34m(question)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mqnorm2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_vector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mq_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq_vector\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mqnorm2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0msim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mq_vector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[1;31m# 使用优先队列找出top5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mpq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPriorityQueue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (17026,100) (100,1) "
     ]
    }
   ],
   "source": [
    "# print(top5results(\"Which airport was shut down?\"))\n",
    "print(top5results_vector(\"Which airport was shut down?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
